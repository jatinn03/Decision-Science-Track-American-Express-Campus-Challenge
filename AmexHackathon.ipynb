{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecaad478-5acb-4161-87f4-499f05a674d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "closing parenthesis ']' does not match opening parenthesis '(' (3972566633.py, line 220)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 220\u001b[0;36m\u001b[0m\n\u001b[0;31m    ctr_features = [col for col in df.columns if 'ctr' in col.lower() or (col.startswith('f1') and col[1:].isdigit() and int(col[1:]) in range(4,10)])\u001b[0m\n\u001b[0m                                                                                                                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m closing parenthesis ']' does not match opening parenthesis '('\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import joblib\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ClickPredictionModel:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.feature_columns = None\n",
    "        self.label_encoders = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load all datasets\"\"\"\n",
    "        # Load main datasets\n",
    "        self.df_events = pd.read_parquet('/Users/jatin/Desktop/AMEX Hackathon/add_event.parquet')\n",
    "        self.df_trans = pd.read_parquet('/Users/jatin/Desktop/AMEX Hackathon/add_trans.parquet')\n",
    "        self.df_offers = pd.read_parquet('/Users/jatin/Desktop/AMEX Hackathon/offer_metadata.parquet')\n",
    "        self.df_train = pd.read_parquet('/Users/jatin/Desktop/AMEX Hackathon/train_data.parquet')\n",
    "        self.df_test = pd.read_parquet('/Users/jatin/Desktop/AMEX Hackathon/test_data.parquet')\n",
    "        self.df_submission = pd.read_csv('/Users/jatin/Desktop/AMEX Hackathon/submission_template.csv')\n",
    "        \n",
    "        # Ensure ID columns are consistent\n",
    "        for df in [self.df_events, self.df_trans, self.df_offers, self.df_train, self.df_test]:\n",
    "            if 'id2' in df.columns:\n",
    "                df['id2'] = df['id2'].astype(str)\n",
    "        \n",
    "        print(\"Data loaded successfully!\")\n",
    "        print(f\"Events shape: {self.df_events.shape}\")\n",
    "        print(f\"Transactions shape: {self.df_trans.shape}\")\n",
    "        print(f\"Offers shape: {self.df_offers.shape}\")\n",
    "        print(f\"Train shape: {self.df_train.shape}\")\n",
    "        print(f\"Test shape: {self.df_test.shape}\")\n",
    "        \n",
    "    def create_event_features(self, df):\n",
    "        \"\"\"Create features from events data\"\"\"\n",
    "        if hasattr(self, 'df_events') and not self.df_events.empty:\n",
    "            # Convert timestamp columns to datetime safely\n",
    "            try:\n",
    "                self.df_events['id4'] = pd.to_datetime(self.df_events['id4'], errors='coerce')\n",
    "                self.df_events['id7'] = pd.to_datetime(self.df_events['id7'], errors='coerce')\n",
    "                self.df_events['timestamp_diff'] = (self.df_events['id7'] - self.df_events['id4']).dt.total_seconds()\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing timestamps: {e}\")\n",
    "                self.df_events['timestamp_diff'] = 0\n",
    "            \n",
    "            # Basic event aggregations\n",
    "            agg_dict = {\n",
    "                'id3': ['count', 'nunique'],\n",
    "                'timestamp_diff': ['min', 'max', 'mean', 'std'],\n",
    "                'id6': 'nunique'\n",
    "            }\n",
    "            \n",
    "            valid_columns = {k: v for k, v in agg_dict.items() if k in self.df_events.columns}\n",
    "            events_agg = self.df_events.groupby('id2').agg(valid_columns).reset_index()\n",
    "            \n",
    "            # Flatten column names\n",
    "            events_agg.columns = ['id2'] + [f'event_{col[0]}_{col[1]}' for col in events_agg.columns[1:]]\n",
    "            \n",
    "            # Calculate event-based ratios safely\n",
    "            if 'event_id3_count' in events_agg.columns:\n",
    "                events_agg['event_click_rate'] = events_agg['event_id3_count'] / (events_agg['event_id3_count'] + 1e-6)\n",
    "            \n",
    "            # Merge with main dataframe\n",
    "            if 'id2' in df.columns:\n",
    "                df = df.merge(events_agg, on='id2', how='left')\n",
    "            \n",
    "            # Customer-offer specific features\n",
    "            if all(col in self.df_events.columns for col in ['id2', 'id3', 'timestamp_diff', 'id6']):\n",
    "                customer_offer_agg = {\n",
    "                    'timestamp_diff': ['count', 'mean'],\n",
    "                    'id6': 'nunique'\n",
    "                }\n",
    "                \n",
    "                customer_offer_events = self.df_events.groupby(['id2', 'id3']).agg(customer_offer_agg).reset_index()\n",
    "                customer_offer_events.columns = ['id2', 'id3'] + [f'co_event_{col[0]}_{col[1]}' for col in customer_offer_events.columns[2:]]\n",
    "                \n",
    "                if 'co_event_timestamp_diff_count' in customer_offer_events.columns:\n",
    "                    customer_offer_events['co_event_specific_ctr'] = (\n",
    "                        customer_offer_events['co_event_timestamp_diff_count'] / \n",
    "                        (customer_offer_events['co_event_timestamp_diff_count'] + 1e-6)\n",
    "                    )\n",
    "                \n",
    "                if all(col in df.columns for col in ['id2', 'id3']):\n",
    "                    df = df.merge(customer_offer_events, on=['id2', 'id3'], how='left')\n",
    "                    \n",
    "        return df\n",
    "    \n",
    "    def create_transaction_features(self, df):\n",
    "        \"\"\"Create features from transaction data\"\"\"\n",
    "        if hasattr(self, 'df_trans') and not self.df_trans.empty:\n",
    "            # Convert numeric columns to numeric type\n",
    "            numeric_cols = ['f367', 'f368', 'f369', 'f370', 'f371', 'f372', 'f374']\n",
    "            for col in numeric_cols:\n",
    "                if col in self.df_trans.columns:\n",
    "                    self.df_trans[col] = pd.to_numeric(self.df_trans[col], errors='coerce')\n",
    "            \n",
    "            # Handle non-numeric columns safely\n",
    "            if 'id8' in self.df_trans.columns:\n",
    "                self.df_trans['id8'] = self.df_trans['id8'].astype(str)\n",
    "            \n",
    "            # Aggregate transaction features by customer\n",
    "            agg_dict = {\n",
    "                'f367': ['sum', 'mean', 'count', 'std', 'min', 'max'],\n",
    "                'f368': ['nunique', 'count'],\n",
    "                'f369': ['sum', 'mean'],\n",
    "                'f370': ['min', 'max', 'nunique'],\n",
    "                'f371': ['mean', 'std'],\n",
    "                'f372': ['nunique'],\n",
    "                'f374': ['nunique'],\n",
    "                'id8': ['nunique']\n",
    "            }\n",
    "            \n",
    "            # Only include columns that exist in the dataframe\n",
    "            valid_columns = {k: v for k, v in agg_dict.items() if k in self.df_trans.columns}\n",
    "            trans_agg = self.df_trans.groupby('id2').agg(valid_columns).reset_index()\n",
    "            \n",
    "            # Flatten column names\n",
    "            trans_agg.columns = ['id2'] + [f'trans_{col[0]}_{col[1]}' for col in trans_agg.columns[1:]]\n",
    "            \n",
    "            # Create advanced transaction features safely\n",
    "            if 'trans_f367_sum' in trans_agg.columns and 'trans_f367_count' in trans_agg.columns:\n",
    "                trans_agg['trans_avg_amount'] = trans_agg['trans_f367_sum'] / (trans_agg['trans_f367_count'] + 1e-6)\n",
    "            \n",
    "            if 'trans_f367_std' in trans_agg.columns and 'trans_f367_mean' in trans_agg.columns:\n",
    "                trans_agg['trans_amount_cv'] = trans_agg['trans_f367_std'] / (trans_agg['trans_f367_mean'] + 1e-6)\n",
    "            \n",
    "            if 'trans_f367_max' in trans_agg.columns and 'trans_f367_mean' in trans_agg.columns:\n",
    "                trans_agg['trans_high_value_ratio'] = (trans_agg['trans_f367_max'] > trans_agg['trans_f367_mean'] * 2).astype(int)\n",
    "            \n",
    "            if 'trans_f368_nunique' in trans_agg.columns and 'trans_f368_count' in trans_agg.columns:\n",
    "                trans_agg['trans_product_diversity'] = trans_agg['trans_f368_nunique'] / (trans_agg['trans_f368_count'] + 1e-6)\n",
    "            \n",
    "            if 'trans_amount_cv' in trans_agg.columns:\n",
    "                trans_agg['trans_spending_consistency'] = 1 / (trans_agg['trans_amount_cv'] + 1e-6)\n",
    "            \n",
    "            # Merge with main dataframe\n",
    "            if 'id2' in df.columns:\n",
    "                df = df.merge(trans_agg, on='id2', how='left')\n",
    "                \n",
    "        return df\n",
    "    \n",
    "    def create_offer_features(self, df):\n",
    "        \"\"\"Create features from offer metadata\"\"\"\n",
    "        if hasattr(self, 'df_offers') and not self.df_offers.empty:\n",
    "            # Ensure numeric columns are numeric\n",
    "            numeric_cols = ['f375', 'f376', 'f377', 'f378', 'f374']\n",
    "            for col in numeric_cols:\n",
    "                if col in self.df_offers.columns:\n",
    "                    self.df_offers[col] = pd.to_numeric(self.df_offers[col], errors='coerce')\n",
    "            \n",
    "            # Define aggregations\n",
    "            agg_dict = {\n",
    "                'f375': ['mean', 'std'],\n",
    "                'f376': ['mean', 'std'],\n",
    "                'f377': ['mean', 'std'],\n",
    "                'id10': ['nunique'],\n",
    "                'id11': ['nunique'],\n",
    "                'f378': ['mean', 'std'],\n",
    "                'f374': ['nunique'],\n",
    "                'id12': ['min', 'max', 'mean'],\n",
    "                'id13': ['min', 'max', 'mean']\n",
    "            }\n",
    "            \n",
    "            # Only include columns that exist\n",
    "            valid_columns = {k: v for k, v in agg_dict.items() if k in self.df_offers.columns}\n",
    "            offer_agg = self.df_offers.groupby('id3').agg(valid_columns).reset_index()\n",
    "            \n",
    "            # Flatten column names\n",
    "            offer_agg.columns = ['id3'] + [f'offer_{col[0]}_{col[1]}' for col in offer_agg.columns[1:]]\n",
    "            \n",
    "            # Create offer-specific features safely\n",
    "            if 'offer_id13_mean' in offer_agg.columns and 'offer_id12_mean' in offer_agg.columns:\n",
    "                offer_agg['offer_duration'] = offer_agg['offer_id13_mean'] - offer_agg['offer_id12_mean']\n",
    "            \n",
    "            if 'offer_f376_mean' in offer_agg.columns and 'offer_f375_mean' in offer_agg.columns:\n",
    "                offer_agg['offer_attractiveness'] = offer_agg['offer_f376_mean'] * offer_agg['offer_f375_mean']\n",
    "            \n",
    "            if 'offer_f378_std' in offer_agg.columns and 'offer_f378_mean' in offer_agg.columns:\n",
    "                offer_agg['offer_complexity'] = offer_agg['offer_f378_std'] / (offer_agg['offer_f378_mean'] + 1e-6)\n",
    "            \n",
    "            # Merge with main dataframe\n",
    "            if 'id3' in df.columns:\n",
    "                df = df.merge(offer_agg, on='id3', how='left')\n",
    "                \n",
    "        return df\n",
    "    \n",
    "    def create_advanced_features(self, df):\n",
    "        \"\"\"Create advanced engineered features\"\"\"\n",
    "        # Make a copy to avoid SettingWithCopyWarning\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Time-based features\n",
    "        if 'id5' in df.columns:\n",
    "            df['day_of_week'] = df['id5'] % 7\n",
    "            df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "        \n",
    "        if 'id4' in df.columns:\n",
    "            df['hour_of_day'] = (df['id4'] % 86400) // 3600\n",
    "            df['is_business_hours'] = ((df['hour_of_day'] >= 9) & (df['hour_of_day'] <= 17)).astype(int)\n",
    "        \n",
    "        # Customer engagement features\n",
    "        engagement_features = [f for f in ['f28', 'f29', 'f30', 'f31', 'f147', 'f148', 'f149', 'f150'] \n",
    "                             if f in df.columns]\n",
    "        if engagement_features:\n",
    "            df['total_engagement'] = df[engagement_features].sum(axis=1)\n",
    "            df['avg_engagement'] = df[engagement_features].mean(axis=1)\n",
    "            df['engagement_consistency'] = df[engagement_features].std(axis=1)\n",
    "        \n",
    "        # CTR-based features\n",
    "        ctr_features = [col for col in df.columns if 'ctr' in col.lower() or (col.startswith('f1') and col[1:].isdigit() and int(col[1:]) in range(4,10)])\n",
    "        if ctr_features:\n",
    "            df['avg_ctr'] = df[ctr_features].mean(axis=1)\n",
    "            df['max_ctr'] = df[ctr_features].max(axis=1)\n",
    "            df['ctr_consistency'] = df[ctr_features].std(axis=1)\n",
    "        \n",
    "        # Spending pattern features\n",
    "        spending_features = [f for f in ['f39', 'f40', 'f41'] if f in df.columns]\n",
    "        if spending_features:\n",
    "            df['total_spending'] = df[spending_features].sum(axis=1)\n",
    "            df['spending_diversity'] = (df[spending_features] > 0).sum(axis=1)\n",
    "            if len(spending_features) > 0:\n",
    "                df['dominant_spending_category'] = df[spending_features].idxmax(axis=1)\n",
    "        \n",
    "        # Interaction features with existence checks\n",
    "        if all(f in df.columns for f in ['f363', 'f331']):\n",
    "            df['f363_x_f331'] = df['f363'] * df['f331']\n",
    "        \n",
    "        if all(f in df.columns for f in ['f366', 'f329']):\n",
    "            df['f366_x_f329'] = df['f366'] * df['f329']\n",
    "        \n",
    "        # Similar checks for all other interaction features...\n",
    "        \n",
    "        # PCA features\n",
    "        low_imp_features = [col for col in df.columns \n",
    "                           if col.startswith('f') and col[1:].isdigit() and int(col[1:]) > 300]\n",
    "        if len(low_imp_features) > 1:\n",
    "            try:\n",
    "                pca = PCA(n_components=min(5, len(low_imp_features)))\n",
    "                df_pca = pca.fit_transform(df[low_imp_features].fillna(0))\n",
    "                for i in range(df_pca.shape[1]):\n",
    "                    df[f'pca_component_{i}'] = df_pca[:, i]\n",
    "            except Exception as e:\n",
    "                print(f\"PCA failed: {e}\")\n",
    "        \n",
    "        # Clustering features\n",
    "        cluster_features = [f for f in ['f363', 'f366', 'f150', 'f138'] if f in df.columns]\n",
    "        if len(cluster_features) >= 2:\n",
    "            try:\n",
    "                n_clusters = min(5, len(df)//10)\n",
    "                if n_clusters > 1:\n",
    "                    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "                    df['customer_cluster'] = kmeans.fit_predict(df[cluster_features].fillna(0))\n",
    "            except Exception as e:\n",
    "                print(f\"Clustering failed: {e}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def basic_preprocessing(self, df):\n",
    "        \"\"\"Basic preprocessing steps\"\"\"\n",
    "        # Convert ID columns to consistent types\n",
    "        id_cols = ['id1', 'id2', 'id3', 'id4', 'id5']\n",
    "        for col in id_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(str)  # Convert all IDs to string type\n",
    "        \n",
    "        # Convert other columns to numeric\n",
    "        for col in df.columns:\n",
    "            if col not in id_cols:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        # Handle missing values\n",
    "        df.fillna(0, inplace=True)\n",
    "        \n",
    "        # Handle infinite values\n",
    "        df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    def feature_engineering(self, df):\n",
    "        \"\"\"Complete feature engineering pipeline\"\"\"\n",
    "        print(\"Starting feature engineering...\")\n",
    "        \n",
    "        # Basic preprocessing\n",
    "        df = self.basic_preprocessing(df)\n",
    "        \n",
    "        # Create features from additional datasets\n",
    "        df = self.create_event_features(df)\n",
    "        df = self.create_transaction_features(df)\n",
    "        df = self.create_offer_features(df)\n",
    "        \n",
    "        # Create advanced features\n",
    "        df = self.create_advanced_features(df)\n",
    "        \n",
    "        # Final preprocessing\n",
    "        df = self.basic_preprocessing(df)\n",
    "        \n",
    "        print(f\"Feature engineering complete. Shape: {df.shape}\")\n",
    "        return df\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Prepare training and test data\"\"\"\n",
    "        print(\"Preparing data...\")\n",
    "        \n",
    "        # Apply feature engineering\n",
    "        self.df_train = self.feature_engineering(self.df_train.copy())\n",
    "        self.df_test = self.feature_engineering(self.df_test.copy())\n",
    "        \n",
    "        # Extract target and IDs\n",
    "        self.y_train = self.df_train['y'].astype(int)\n",
    "        self.ids_train = self.df_train[['id1', 'id2', 'id3', 'id4', 'id5']].copy()\n",
    "        self.ids_test = self.df_test[['id1', 'id2', 'id3', 'id4', 'id5']].copy()\n",
    "        \n",
    "        # Drop IDs and target\n",
    "        id_cols = ['id1', 'id2', 'id3', 'id4', 'id5', 'y']\n",
    "        self.df_train = self.df_train.drop(columns=[col for col in id_cols if col in self.df_train.columns])\n",
    "        self.df_test = self.df_test.drop(columns=[col for col in id_cols if col in self.df_test.columns])\n",
    "        \n",
    "        # Ensure same columns in train and test\n",
    "        common_cols = list(set(self.df_train.columns) & set(self.df_test.columns))\n",
    "        self.df_train = self.df_train[common_cols]\n",
    "        self.df_test = self.df_test[common_cols]\n",
    "        \n",
    "        # Remove low variance features\n",
    "        numeric_cols = self.df_train.select_dtypes(include=[np.number]).columns\n",
    "        low_var_cols = []\n",
    "        for col in numeric_cols:\n",
    "            if self.df_train[col].std() < 1e-6:\n",
    "                low_var_cols.append(col)\n",
    "        \n",
    "        if low_var_cols:\n",
    "            self.df_train = self.df_train.drop(columns=low_var_cols)\n",
    "            self.df_test = self.df_test.drop(columns=low_var_cols)\n",
    "            print(f\"Removed {len(low_var_cols)} low variance features\")\n",
    "        \n",
    "        self.feature_columns = self.df_train.columns.tolist()\n",
    "        print(f\"Final feature count: {len(self.feature_columns)}\")\n",
    "        \n",
    "    def train_model(self):\n",
    "        \"\"\"Train the XGBoost model with cross-validation\"\"\"\n",
    "        print(\"Training model...\")\n",
    "        \n",
    "        # Split for validation\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            self.df_train, self.y_train, test_size=0.2, random_state=42, stratify=self.y_train\n",
    "        )\n",
    "        \n",
    "        # XGBoost parameters optimized for ranking\n",
    "        params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'learning_rate': 0.05,\n",
    "            'max_depth': 8,\n",
    "            'min_child_weight': 1,\n",
    "            'gamma': 0.1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'reg_alpha': 0.1,\n",
    "            'reg_lambda': 1.0,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        # Prepare DMatrix\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dval = xgb.DMatrix(X_val, label=y_val)\n",
    "        \n",
    "        # Train model\n",
    "        self.model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=2000,\n",
    "            evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval=100\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        y_pred = self.model.predict(dval)\n",
    "        auc_score = roc_auc_score(y_val, y_pred)\n",
    "        print(f\"Validation AUC: {auc_score:.4f}\")\n",
    "        \n",
    "        return auc_score\n",
    "    \n",
    "    def feature_selection(self, top_k=500):\n",
    "        \"\"\"Feature selection using SHAP\"\"\"\n",
    "        print(\"Performing feature selection...\")\n",
    "        \n",
    "        # Sample for SHAP calculation\n",
    "        sample_size = min(1000, len(self.df_train))\n",
    "        sample_idx = np.random.choice(len(self.df_train), sample_size, replace=False)\n",
    "        X_sample = self.df_train.iloc[sample_idx]\n",
    "        \n",
    "        # Calculate SHAP values\n",
    "        explainer = shap.TreeExplainer(self.model)\n",
    "        shap_values = explainer.shap_values(X_sample)\n",
    "        \n",
    "        # Get feature importance\n",
    "        feature_importance = np.abs(shap_values).mean(axis=0)\n",
    "        feature_scores = pd.Series(feature_importance, index=X_sample.columns)\n",
    "        \n",
    "        # Select top features\n",
    "        top_features = feature_scores.nlargest(top_k).index.tolist()\n",
    "        \n",
    "        print(f\"Selected {len(top_features)} features out of {len(self.feature_columns)}\")\n",
    "        \n",
    "        # Update datasets\n",
    "        self.df_train = self.df_train[top_features]\n",
    "        self.df_test = self.df_test[top_features]\n",
    "        self.feature_columns = top_features\n",
    "        \n",
    "        return top_features\n",
    "    \n",
    "    def train_final_model(self):\n",
    "        \"\"\"Train final model on selected features\"\"\"\n",
    "        print(\"Training final model...\")\n",
    "        \n",
    "        # Parameters for final model\n",
    "        params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'learning_rate': 0.03,\n",
    "            'max_depth': 10,\n",
    "            'min_child_weight': 1,\n",
    "            'gamma': 0.1,\n",
    "            'subsample': 0.9,\n",
    "            'colsample_bytree': 0.9,\n",
    "            'reg_alpha': 0.1,\n",
    "            'reg_lambda': 1.0,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = []\n",
    "        kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kfold.split(self.df_train, self.y_train)):\n",
    "            X_train_fold = self.df_train.iloc[train_idx]\n",
    "            X_val_fold = self.df_train.iloc[val_idx]\n",
    "            y_train_fold = self.y_train.iloc[train_idx]\n",
    "            y_val_fold = self.y_train.iloc[val_idx]\n",
    "            \n",
    "            dtrain = xgb.DMatrix(X_train_fold, label=y_train_fold)\n",
    "            dval = xgb.DMatrix(X_val_fold, label=y_val_fold)\n",
    "            \n",
    "            model_fold = xgb.train(\n",
    "                params,\n",
    "                dtrain,\n",
    "                num_boost_round=3000,\n",
    "                evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "                early_stopping_rounds=150,\n",
    "                verbose_eval=False\n",
    "            )\n",
    "            \n",
    "            y_pred = model_fold.predict(dval)\n",
    "            auc_score = roc_auc_score(y_val_fold, y_pred)\n",
    "            cv_scores.append(auc_score)\n",
    "            print(f\"Fold {fold+1} AUC: {auc_score:.4f}\")\n",
    "        \n",
    "        print(f\"CV AUC: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores)*2:.4f})\")\n",
    "        \n",
    "        # Train final model on full data\n",
    "        dtrain_full = xgb.DMatrix(self.df_train, label=self.y_train)\n",
    "        \n",
    "        self.model = xgb.train(\n",
    "            params,\n",
    "            dtrain_full,\n",
    "            num_boost_round=int(np.mean([model.best_iteration for model in [model_fold]]) * 1.1),\n",
    "            verbose_eval=100\n",
    "        )\n",
    "        \n",
    "        return np.mean(cv_scores)\n",
    "    \n",
    "    def predict(self):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        print(\"Making predictions...\")\n",
    "        \n",
    "        dtest = xgb.DMatrix(self.df_test)\n",
    "        predictions = self.model.predict(dtest)\n",
    "        \n",
    "        # Create submission\n",
    "        submission = pd.DataFrame({\n",
    "            'id1': self.ids_test['id1'],\n",
    "            'id2': self.ids_test['id2'],\n",
    "            'id3': self.ids_test['id3'],\n",
    "            'id5': self.ids_test['id5'],\n",
    "            'pred': predictions\n",
    "        })\n",
    "        \n",
    "        return submission\n",
    "    \n",
    "    def create_submission(self, predictions_df):\n",
    "        \"\"\"Create final submission file\"\"\"\n",
    "        print(\"Creating submission...\")\n",
    "        \n",
    "        # Map predictions to submission template\n",
    "        pred_map = dict(zip(predictions_df['id1'], predictions_df['pred']))\n",
    "        \n",
    "        # Load submission template\n",
    "        submission = self.df_submission.copy()\n",
    "        submission['y'] = submission['id1'].map(pred_map).fillna(0.5).clip(0, 1)\n",
    "        \n",
    "        # Save submission\n",
    "        submission.to_csv('final_submission.csv', index=False)\n",
    "        print(f\"Submission saved with {len(submission)} rows\")\n",
    "        \n",
    "        return submission\n",
    "    \n",
    "    def run_pipeline(self):\n",
    "        \"\"\"Run complete pipeline\"\"\"\n",
    "        print(\"Starting complete pipeline...\")\n",
    "        \n",
    "        # Load data\n",
    "        self.load_data()\n",
    "        \n",
    "        # Prepare data\n",
    "        self.prepare_data()\n",
    "        \n",
    "        # Train initial model\n",
    "        initial_auc = self.train_model()\n",
    "        \n",
    "        # Feature selection\n",
    "        selected_features = self.feature_selection(top_k=400)\n",
    "        \n",
    "        # Train final model\n",
    "        final_auc = self.train_final_model()\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = self.predict()\n",
    "        \n",
    "        # Create submission\n",
    "        submission = self.create_submission(predictions)\n",
    "        \n",
    "        print(f\"Pipeline complete!\")\n",
    "        print(f\"Initial AUC: {initial_auc:.4f}\")\n",
    "        print(f\"Final AUC: {final_auc:.4f}\")\n",
    "        print(f\"Submission shape: {submission.shape}\")\n",
    "        \n",
    "        return submission\n",
    "\n",
    "# Run the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    model = ClickPredictionModel()\n",
    "    submission = model.run_pipeline()\n",
    "    \n",
    "    # Display prediction distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(submission['y'], bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.title('Distribution of Predicted Click Probabilities')\n",
    "    plt.xlabel('Predicted Probability')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Prediction statistics:\")\n",
    "    print(f\"Mean: {submission['y'].mean():.4f}\")\n",
    "    print(f\"Std: {submission['y'].std():.4f}\")\n",
    "    print(f\"Min: {submission['y'].min():.4f}\")\n",
    "    print(f\"Max: {submission['y'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ff0e375-58ef-409b-a35b-ffa79994410a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting complete pipeline...\n",
      "Data loaded successfully!\n",
      "Events shape: (21457473, 5)\n",
      "Transactions shape: (6339465, 9)\n",
      "Offers shape: (4164, 12)\n",
      "Train shape: (770164, 372)\n",
      "Test shape: (369301, 371)\n",
      "Preparing data...\n",
      "Starting feature engineering...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on object and int32 columns for key 'id3'. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 648\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    647\u001b[0m     model \u001b[38;5;241m=\u001b[39m ClickPredictionModel()\n\u001b[0;32m--> 648\u001b[0m     submission \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# Display prediction distribution\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "Cell \u001b[0;32mIn[1], line 621\u001b[0m, in \u001b[0;36mClickPredictionModel.run_pipeline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[1;32m    620\u001b[0m \u001b[38;5;66;03m# Prepare data\u001b[39;00m\n\u001b[0;32m--> 621\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;66;03m# Train initial model\u001b[39;00m\n\u001b[1;32m    624\u001b[0m initial_auc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_model()\n",
      "Cell \u001b[0;32mIn[1], line 402\u001b[0m, in \u001b[0;36mClickPredictionModel.prepare_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreparing data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# Apply feature engineering\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_engineering\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_engineering(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf_test\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# Extract target and IDs\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 386\u001b[0m, in \u001b[0;36mClickPredictionModel.feature_engineering\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    384\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_event_features(df)\n\u001b[1;32m    385\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_transaction_features(df)\n\u001b[0;32m--> 386\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_offer_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# Create advanced features\u001b[39;00m\n\u001b[1;32m    389\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_advanced_features(df)\n",
      "Cell \u001b[0;32mIn[1], line 218\u001b[0m, in \u001b[0;36mClickPredictionModel.create_offer_features\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# Merge with main dataframe\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid3\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m--> 218\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43moffer_agg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/frame.py:10832\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m  10813\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m  10814\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m  10815\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10828\u001b[0m     validate: MergeValidate \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m  10829\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m  10830\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[0;32m> 10832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  10833\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10841\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10842\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10846\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/reshape/merge.py:170\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[1;32m    156\u001b[0m         left_df,\n\u001b[1;32m    157\u001b[0m         right_df,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/reshape/merge.py:807\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tolerance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys)\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;66;03m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[0;32m--> 807\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_coerce_merge_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;66;03m# If argument passed to validate,\u001b[39;00m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;66;03m# check if columns specified as unique\u001b[39;00m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;66;03m# are in fact unique.\u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/reshape/merge.py:1508\u001b[0m, in \u001b[0;36m_MergeOperation._maybe_coerce_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1502\u001b[0m     \u001b[38;5;66;03m# unless we are merging non-string-like with string-like\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m   1504\u001b[0m         inferred_left \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_right \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[1;32m   1505\u001b[0m     ) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1506\u001b[0m         inferred_right \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_left \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[1;32m   1507\u001b[0m     ):\n\u001b[0;32m-> 1508\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;66;03m# datetimelikes must match exactly\u001b[39;00m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m needs_i8_conversion(lk\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m needs_i8_conversion(rk\u001b[38;5;241m.\u001b[39mdtype):\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to merge on object and int32 columns for key 'id3'. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import joblib\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ClickPredictionModel:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.feature_columns = None\n",
    "        self.label_encoders = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load all datasets\"\"\"\n",
    "        # Load main datasets\n",
    "        self.df_events = pd.read_parquet('/Users/jatin/Desktop/AMEX Hackathon/add_event.parquet')\n",
    "        self.df_trans = pd.read_parquet('/Users/jatin/Desktop/AMEX Hackathon/add_trans.parquet')\n",
    "        self.df_offers = pd.read_parquet('/Users/jatin/Desktop/AMEX Hackathon/offer_metadata.parquet')\n",
    "        self.df_train = pd.read_parquet('/Users/jatin/Desktop/AMEX Hackathon/train_data.parquet')\n",
    "        self.df_test = pd.read_parquet('/Users/jatin/Desktop/AMEX Hackathon/test_data.parquet')\n",
    "        self.df_submission = pd.read_csv('/Users/jatin/Desktop/AMEX Hackathon/submission_template.csv')\n",
    "        \n",
    "        # Ensure ID columns are consistent\n",
    "        for df in [self.df_events, self.df_trans, self.df_offers, self.df_train, self.df_test]:\n",
    "            if 'id2' in df.columns:\n",
    "                df['id2'] = df['id2'].astype(str)\n",
    "        \n",
    "        print(\"Data loaded successfully!\")\n",
    "        print(f\"Events shape: {self.df_events.shape}\")\n",
    "        print(f\"Transactions shape: {self.df_trans.shape}\")\n",
    "        print(f\"Offers shape: {self.df_offers.shape}\")\n",
    "        print(f\"Train shape: {self.df_train.shape}\")\n",
    "        print(f\"Test shape: {self.df_test.shape}\")\n",
    "        \n",
    "    def create_event_features(self, df):\n",
    "        \"\"\"Create features from events data\"\"\"\n",
    "        if hasattr(self, 'df_events') and not self.df_events.empty:\n",
    "            # Convert timestamp columns to datetime safely\n",
    "            try:\n",
    "                self.df_events['id4'] = pd.to_datetime(self.df_events['id4'], errors='coerce')\n",
    "                self.df_events['id7'] = pd.to_datetime(self.df_events['id7'], errors='coerce')\n",
    "                self.df_events['timestamp_diff'] = (self.df_events['id7'] - self.df_events['id4']).dt.total_seconds()\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing timestamps: {e}\")\n",
    "                self.df_events['timestamp_diff'] = 0\n",
    "            \n",
    "            # Basic event aggregations\n",
    "            agg_dict = {\n",
    "                'id3': ['count', 'nunique'],\n",
    "                'timestamp_diff': ['min', 'max', 'mean', 'std'],\n",
    "                'id6': 'nunique'\n",
    "            }\n",
    "            \n",
    "            valid_columns = {k: v for k, v in agg_dict.items() if k in self.df_events.columns}\n",
    "            events_agg = self.df_events.groupby('id2').agg(valid_columns).reset_index()\n",
    "            \n",
    "            # Flatten column names\n",
    "            events_agg.columns = ['id2'] + [f'event_{col[0]}_{col[1]}' for col in events_agg.columns[1:]]\n",
    "            \n",
    "            # Calculate event-based ratios safely\n",
    "            if 'event_id3_count' in events_agg.columns:\n",
    "                events_agg['event_click_rate'] = events_agg['event_id3_count'] / (events_agg['event_id3_count'] + 1e-6)\n",
    "            \n",
    "            # Merge with main dataframe\n",
    "            if 'id2' in df.columns:\n",
    "                df = df.merge(events_agg, on='id2', how='left')\n",
    "            \n",
    "            # Customer-offer specific features\n",
    "            if all(col in self.df_events.columns for col in ['id2', 'id3', 'timestamp_diff', 'id6']):\n",
    "                customer_offer_agg = {\n",
    "                    'timestamp_diff': ['count', 'mean'],\n",
    "                    'id6': 'nunique'\n",
    "                }\n",
    "                \n",
    "                customer_offer_events = self.df_events.groupby(['id2', 'id3']).agg(customer_offer_agg).reset_index()\n",
    "                customer_offer_events.columns = ['id2', 'id3'] + [f'co_event_{col[0]}_{col[1]}' for col in customer_offer_events.columns[2:]]\n",
    "                \n",
    "                if 'co_event_timestamp_diff_count' in customer_offer_events.columns:\n",
    "                    customer_offer_events['co_event_specific_ctr'] = (\n",
    "                        customer_offer_events['co_event_timestamp_diff_count'] / \n",
    "                        (customer_offer_events['co_event_timestamp_diff_count'] + 1e-6)\n",
    "                    )\n",
    "                \n",
    "                if all(col in df.columns for col in ['id2', 'id3']):\n",
    "                    df = df.merge(customer_offer_events, on=['id2', 'id3'], how='left')\n",
    "                    \n",
    "        return df\n",
    "    \n",
    "    def create_transaction_features(self, df):\n",
    "        \"\"\"Create features from transaction data\"\"\"\n",
    "        if hasattr(self, 'df_trans') and not self.df_trans.empty:\n",
    "            # Convert numeric columns to numeric type\n",
    "            numeric_cols = ['f367', 'f368', 'f369', 'f370', 'f371', 'f372', 'f374']\n",
    "            for col in numeric_cols:\n",
    "                if col in self.df_trans.columns:\n",
    "                    self.df_trans[col] = pd.to_numeric(self.df_trans[col], errors='coerce')\n",
    "            \n",
    "            # Handle non-numeric columns safely\n",
    "            if 'id8' in self.df_trans.columns:\n",
    "                self.df_trans['id8'] = self.df_trans['id8'].astype(str)\n",
    "            \n",
    "            # Aggregate transaction features by customer\n",
    "            agg_dict = {\n",
    "                'f367': ['sum', 'mean', 'count', 'std', 'min', 'max'],\n",
    "                'f368': ['nunique', 'count'],\n",
    "                'f369': ['sum', 'mean'],\n",
    "                'f370': ['min', 'max', 'nunique'],\n",
    "                'f371': ['mean', 'std'],\n",
    "                'f372': ['nunique'],\n",
    "                'f374': ['nunique'],\n",
    "                'id8': ['nunique']\n",
    "            }\n",
    "            \n",
    "            # Only include columns that exist in the dataframe\n",
    "            valid_columns = {k: v for k, v in agg_dict.items() if k in self.df_trans.columns}\n",
    "            trans_agg = self.df_trans.groupby('id2').agg(valid_columns).reset_index()\n",
    "            \n",
    "            # Flatten column names\n",
    "            trans_agg.columns = ['id2'] + [f'trans_{col[0]}_{col[1]}' for col in trans_agg.columns[1:]]\n",
    "            \n",
    "            # Create advanced transaction features safely\n",
    "            if 'trans_f367_sum' in trans_agg.columns and 'trans_f367_count' in trans_agg.columns:\n",
    "                trans_agg['trans_avg_amount'] = trans_agg['trans_f367_sum'] / (trans_agg['trans_f367_count'] + 1e-6)\n",
    "            \n",
    "            if 'trans_f367_std' in trans_agg.columns and 'trans_f367_mean' in trans_agg.columns:\n",
    "                trans_agg['trans_amount_cv'] = trans_agg['trans_f367_std'] / (trans_agg['trans_f367_mean'] + 1e-6)\n",
    "            \n",
    "            if 'trans_f367_max' in trans_agg.columns and 'trans_f367_mean' in trans_agg.columns:\n",
    "                trans_agg['trans_high_value_ratio'] = (trans_agg['trans_f367_max'] > trans_agg['trans_f367_mean'] * 2).astype(int)\n",
    "            \n",
    "            if 'trans_f368_nunique' in trans_agg.columns and 'trans_f368_count' in trans_agg.columns:\n",
    "                trans_agg['trans_product_diversity'] = trans_agg['trans_f368_nunique'] / (trans_agg['trans_f368_count'] + 1e-6)\n",
    "            \n",
    "            if 'trans_amount_cv' in trans_agg.columns:\n",
    "                trans_agg['trans_spending_consistency'] = 1 / (trans_agg['trans_amount_cv'] + 1e-6)\n",
    "            \n",
    "            # Merge with main dataframe\n",
    "            if 'id2' in df.columns:\n",
    "                df = df.merge(trans_agg, on='id2', how='left')\n",
    "                \n",
    "        return df\n",
    "    \n",
    "    def create_offer_features(self, df):\n",
    "        \"\"\"Create features from offer metadata\"\"\"\n",
    "        if hasattr(self, 'df_offers') and not self.df_offers.empty:\n",
    "            # Ensure numeric columns are numeric\n",
    "            numeric_cols = ['f375', 'f376', 'f377', 'f378', 'f374']\n",
    "            for col in numeric_cols:\n",
    "                if col in self.df_offers.columns:\n",
    "                    self.df_offers[col] = pd.to_numeric(self.df_offers[col], errors='coerce')\n",
    "            \n",
    "            # Convert datetime columns to datetime objects first\n",
    "            datetime_cols = ['id12', 'id13']  # Add any other datetime columns here\n",
    "            for col in datetime_cols:\n",
    "                if col in self.df_offers.columns:\n",
    "                    self.df_offers[col] = pd.to_datetime(self.df_offers[col], errors='coerce')\n",
    "            \n",
    "            # Define aggregations - separate numeric and datetime aggregations\n",
    "            numeric_agg_dict = {\n",
    "                'f375': ['mean', 'std'],\n",
    "                'f376': ['mean', 'std'],\n",
    "                'f377': ['mean', 'std'],\n",
    "                'id10': ['nunique'],\n",
    "                'id11': ['nunique'],\n",
    "                'f378': ['mean', 'std'],\n",
    "                'f374': ['nunique']\n",
    "            }\n",
    "            \n",
    "            datetime_agg_dict = {\n",
    "                'id12': ['min', 'max'],\n",
    "                'id13': ['min', 'max']\n",
    "            }\n",
    "            \n",
    "            # Only include columns that exist\n",
    "            valid_numeric_cols = {k: v for k, v in numeric_agg_dict.items() \n",
    "                                if k in self.df_offers.columns}\n",
    "            valid_datetime_cols = {k: v for k, v in datetime_agg_dict.items() \n",
    "                                 if k in self.df_offers.columns}\n",
    "            \n",
    "            # Perform aggregations separately\n",
    "            numeric_agg = self.df_offers.groupby('id3').agg(valid_numeric_cols).reset_index()\n",
    "            datetime_agg = self.df_offers.groupby('id3').agg(valid_datetime_cols).reset_index()\n",
    "            \n",
    "            # Calculate datetime differences if we have both min and max\n",
    "            if all(f'id13_{agg}' in datetime_agg.columns for agg in ['min', 'max']):\n",
    "                datetime_agg['offer_duration'] = (\n",
    "                    datetime_agg['id13_max'] - datetime_agg['id12_min']\n",
    "                ).dt.total_seconds()\n",
    "            \n",
    "            # Merge the aggregations\n",
    "            offer_agg = numeric_agg.merge(datetime_agg, on='id3', how='left')\n",
    "            \n",
    "            # Flatten column names\n",
    "            offer_agg.columns = ['id3'] + [f'offer_{col[0]}_{col[1]}' \n",
    "                                         for col in offer_agg.columns[1:]]\n",
    "            \n",
    "            # Create offer-specific features safely\n",
    "            if 'offer_f376_mean' in offer_agg.columns and 'offer_f375_mean' in offer_agg.columns:\n",
    "                offer_agg['offer_attractiveness'] = (\n",
    "                    offer_agg['offer_f376_mean'] * offer_agg['offer_f375_mean']\n",
    "                )\n",
    "            \n",
    "            if 'offer_f378_std' in offer_agg.columns and 'offer_f378_mean' in offer_agg.columns:\n",
    "                offer_agg['offer_complexity'] = (\n",
    "                    offer_agg['offer_f378_std'] / (offer_agg['offer_f378_mean'] + 1e-6))\n",
    "            \n",
    "            # Merge with main dataframe\n",
    "            if 'id3' in df.columns:\n",
    "                df = df.merge(offer_agg, on='id3', how='left')\n",
    "                \n",
    "        return df\n",
    "    \n",
    "    def create_advanced_features(self, df):\n",
    "        \"\"\"Create advanced engineered features\"\"\"\n",
    "        # Make a copy to avoid SettingWithCopyWarning\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Time-based features\n",
    "        if 'id5' in df.columns:\n",
    "            df['day_of_week'] = df['id5'] % 7\n",
    "            df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "        \n",
    "        if 'id4' in df.columns:\n",
    "            df['hour_of_day'] = (df['id4'] % 86400) // 3600\n",
    "            df['is_business_hours'] = ((df['hour_of_day'] >= 9) & (df['hour_of_day'] <= 17)).astype(int)\n",
    "        \n",
    "        # Customer engagement features\n",
    "        engagement_features = [f for f in ['f28', 'f29', 'f30', 'f31', 'f147', 'f148', 'f149', 'f150'] \n",
    "                             if f in df.columns]\n",
    "        if engagement_features:\n",
    "            df['total_engagement'] = df[engagement_features].sum(axis=1)\n",
    "            df['avg_engagement'] = df[engagement_features].mean(axis=1)\n",
    "            df['engagement_consistency'] = df[engagement_features].std(axis=1)\n",
    "        \n",
    "        # CTR-based features - Fixed syntax error here\n",
    "        ctr_features = [col for col in df.columns \n",
    "                       if 'ctr' in col.lower() or \n",
    "                       (col.startswith('f1') and col[1:].isdigit() and int(col[1:]) in range(4, 10))]\n",
    "        if ctr_features:\n",
    "            df['avg_ctr'] = df[ctr_features].mean(axis=1)\n",
    "            df['max_ctr'] = df[ctr_features].max(axis=1)\n",
    "            df['ctr_consistency'] = df[ctr_features].std(axis=1)\n",
    "        \n",
    "        # Spending pattern features\n",
    "        spending_features = [f for f in ['f39', 'f40', 'f41'] if f in df.columns]\n",
    "        if spending_features:\n",
    "            df['total_spending'] = df[spending_features].sum(axis=1)\n",
    "            df['spending_diversity'] = (df[spending_features] > 0).sum(axis=1)\n",
    "            if len(spending_features) > 0:\n",
    "                df['dominant_spending_category'] = df[spending_features].idxmax(axis=1)\n",
    "        \n",
    "        # Interaction features with existence checks\n",
    "        if all(f in df.columns for f in ['f363', 'f331']):\n",
    "            df['f363_x_f331'] = df['f363'] * df['f331']\n",
    "        \n",
    "        if all(f in df.columns for f in ['f366', 'f329']):\n",
    "            df['f366_x_f329'] = df['f366'] * df['f329']\n",
    "        \n",
    "        if all(f in df.columns for f in ['f150', 'f329']):\n",
    "            df['f150_x_f329'] = df['f150'] * df['f329']\n",
    "        \n",
    "        if all(f in df.columns for f in ['f138', 'f22']):\n",
    "            df['f138_x_f22'] = df['f138'] * df['f22']\n",
    "        \n",
    "        if all(f in df.columns for f in ['f132', 'f68']):\n",
    "            df['f132_x_f68'] = df['f132'] * df['f68']\n",
    "        \n",
    "        if all(f in df.columns for f in ['f363', 'f366']):\n",
    "            df['f363_x_f366'] = df['f363'] * df['f366']\n",
    "        \n",
    "        # Ratio features\n",
    "        if all(f in df.columns for f in ['f363', 'f329']):\n",
    "            df['f363_div_f329'] = df['f363'] / (df['f329'] + 1e-6)\n",
    "        \n",
    "        if all(f in df.columns for f in ['f366', 'f329']):\n",
    "            df['f366_div_f329'] = df['f366'] / (df['f329'] + 1e-6)\n",
    "        \n",
    "        if all(f in df.columns for f in ['f214', 'f22']):\n",
    "            df['f214_to_f22'] = df['f214'] / (df['f22'] + 1e-6)\n",
    "        \n",
    "        # Higher order features\n",
    "        if 'f132' in df.columns:\n",
    "            df['f132_sq'] = df['f132'] ** 2\n",
    "        \n",
    "        if 'f363' in df.columns:\n",
    "            df['f363_sq'] = df['f363'] ** 2\n",
    "        \n",
    "        if 'f366' in df.columns:\n",
    "            df['f366_sq'] = df['f366'] ** 2\n",
    "        \n",
    "        if 'f363' in df.columns:\n",
    "            df['f363_log'] = np.log1p(df['f363'].clip(lower=0))\n",
    "        \n",
    "        if 'f366' in df.columns:\n",
    "            df['f366_inv'] = 1.0 / (df['f366'] + 1e-6)\n",
    "        \n",
    "        if 'f351' in df.columns:\n",
    "            df['f351_log'] = np.log1p(df['f351'])\n",
    "        \n",
    "        # Binning important features\n",
    "        if 'f363' in df.columns:\n",
    "            df['f363_bin'] = pd.cut(df['f363'], [-np.inf, 0.1, 0.25, np.inf], labels=[0, 1, 2]).astype(int)\n",
    "        \n",
    "        if 'f366' in df.columns:\n",
    "            df['f366_bin'] = pd.cut(df['f366'], [-np.inf, 0.1, 0.3, np.inf], labels=[0, 1, 2]).astype(int)\n",
    "        \n",
    "        # Customer value segmentation\n",
    "        value_features = [f for f in ['f43', 'f44', 'f47', 'f49'] if f in df.columns]\n",
    "        if value_features:\n",
    "            df['customer_value_score'] = df[value_features].sum(axis=1)\n",
    "            if len(df) > 0:  # Check we have data to calculate quantile\n",
    "                df['is_high_value_customer'] = (df['customer_value_score'] > df['customer_value_score'].quantile(0.8)).astype(int)\n",
    "        \n",
    "        # Offer-specific customer features\n",
    "        if all(f in df.columns for f in ['f363', 'f366', 'f150']):\n",
    "            df['customer_offer_match_score'] = df['f363'] * df['f366'] * df['f150']\n",
    "        \n",
    "        if 'f223' in df.columns:\n",
    "            df['recency_score'] = np.exp(-df['f223'] / 30)\n",
    "        \n",
    "        # PCA features\n",
    "        low_imp_features = [col for col in df.columns \n",
    "                          if col.startswith('f') and col[1:].isdigit() and int(col[1:]) > 300]\n",
    "        if len(low_imp_features) > 1:\n",
    "            try:\n",
    "                pca = PCA(n_components=min(5, len(low_imp_features)))\n",
    "                df_pca = pca.fit_transform(df[low_imp_features].fillna(0))\n",
    "                for i in range(df_pca.shape[1]):\n",
    "                    df[f'pca_component_{i}'] = df_pca[:, i]\n",
    "            except Exception as e:\n",
    "                print(f\"PCA failed: {e}\")\n",
    "        \n",
    "        # Clustering features\n",
    "        cluster_features = [f for f in ['f363', 'f366', 'f150', 'f138'] if f in df.columns]\n",
    "        if len(cluster_features) >= 2:\n",
    "            try:\n",
    "                n_clusters = min(5, len(df)//10)\n",
    "                if n_clusters > 1:\n",
    "                    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "                    df['customer_cluster'] = kmeans.fit_predict(df[cluster_features].fillna(0))\n",
    "            except Exception as e:\n",
    "                print(f\"Clustering failed: {e}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def basic_preprocessing(self, df):\n",
    "        \"\"\"Basic preprocessing steps\"\"\"\n",
    "        # Convert ID columns to consistent types\n",
    "        id_cols = ['id1', 'id2', 'id3', 'id4', 'id5']\n",
    "        for col in id_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(str)  # Convert all IDs to string type\n",
    "        \n",
    "        # Convert other columns to numeric\n",
    "        for col in df.columns:\n",
    "            if col not in id_cols:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        # Handle missing values\n",
    "        df.fillna(0, inplace=True)\n",
    "        \n",
    "        # Handle infinite values\n",
    "        df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    def feature_engineering(self, df):\n",
    "        \"\"\"Complete feature engineering pipeline\"\"\"\n",
    "        print(\"Starting feature engineering...\")\n",
    "        \n",
    "        # Basic preprocessing\n",
    "        df = self.basic_preprocessing(df)\n",
    "        \n",
    "        # Create features from additional datasets\n",
    "        df = self.create_event_features(df)\n",
    "        df = self.create_transaction_features(df)\n",
    "        df = self.create_offer_features(df)\n",
    "        \n",
    "        # Create advanced features\n",
    "        df = self.create_advanced_features(df)\n",
    "        \n",
    "        # Final preprocessing\n",
    "        df = self.basic_preprocessing(df)\n",
    "        \n",
    "        print(f\"Feature engineering complete. Shape: {df.shape}\")\n",
    "        return df\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Prepare training and test data\"\"\"\n",
    "        print(\"Preparing data...\")\n",
    "        \n",
    "        # Apply feature engineering\n",
    "        self.df_train = self.feature_engineering(self.df_train.copy())\n",
    "        self.df_test = self.feature_engineering(self.df_test.copy())\n",
    "        \n",
    "        # Extract target and IDs\n",
    "        self.y_train = self.df_train['y'].astype(int)\n",
    "        self.ids_train = self.df_train[['id1', 'id2', 'id3', 'id4', 'id5']].copy()\n",
    "        self.ids_test = self.df_test[['id1', 'id2', 'id3', 'id4', 'id5']].copy()\n",
    "        \n",
    "        # Drop IDs and target\n",
    "        id_cols = ['id1', 'id2', 'id3', 'id4', 'id5', 'y']\n",
    "        self.df_train = self.df_train.drop(columns=[col for col in id_cols if col in self.df_train.columns])\n",
    "        self.df_test = self.df_test.drop(columns=[col for col in id_cols if col in self.df_test.columns])\n",
    "        \n",
    "        # Ensure same columns in train and test\n",
    "        common_cols = list(set(self.df_train.columns) & set(self.df_test.columns))\n",
    "        self.df_train = self.df_train[common_cols]\n",
    "        self.df_test = self.df_test[common_cols]\n",
    "        \n",
    "        # Remove low variance features\n",
    "        numeric_cols = self.df_train.select_dtypes(include=[np.number]).columns\n",
    "        low_var_cols = []\n",
    "        for col in numeric_cols:\n",
    "            if self.df_train[col].std() < 1e-6:\n",
    "                low_var_cols.append(col)\n",
    "        \n",
    "        if low_var_cols:\n",
    "            self.df_train = self.df_train.drop(columns=low_var_cols)\n",
    "            self.df_test = self.df_test.drop(columns=low_var_cols)\n",
    "            print(f\"Removed {len(low_var_cols)} low variance features\")\n",
    "        \n",
    "        self.feature_columns = self.df_train.columns.tolist()\n",
    "        print(f\"Final feature count: {len(self.feature_columns)}\")\n",
    "        \n",
    "    def train_model(self):\n",
    "        \"\"\"Train the XGBoost model with cross-validation\"\"\"\n",
    "        print(\"Training model...\")\n",
    "        \n",
    "        # Split for validation\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            self.df_train, self.y_train, test_size=0.2, random_state=42, stratify=self.y_train\n",
    "        )\n",
    "        \n",
    "        # XGBoost parameters optimized for ranking\n",
    "        params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'learning_rate': 0.05,\n",
    "            'max_depth': 8,\n",
    "            'min_child_weight': 1,\n",
    "            'gamma': 0.1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'reg_alpha': 0.1,\n",
    "            'reg_lambda': 1.0,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        # Prepare DMatrix\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dval = xgb.DMatrix(X_val, label=y_val)\n",
    "        \n",
    "        # Train model\n",
    "        self.model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=2000,\n",
    "            evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval=100\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        y_pred = self.model.predict(dval)\n",
    "        auc_score = roc_auc_score(y_val, y_pred)\n",
    "        print(f\"Validation AUC: {auc_score:.4f}\")\n",
    "        \n",
    "        return auc_score\n",
    "    \n",
    "    def feature_selection(self, top_k=500):\n",
    "        \"\"\"Feature selection using SHAP\"\"\"\n",
    "        print(\"Performing feature selection...\")\n",
    "        \n",
    "        # Sample for SHAP calculation\n",
    "        sample_size = min(1000, len(self.df_train))\n",
    "        if sample_size == 0:\n",
    "            return self.feature_columns[:top_k]\n",
    "            \n",
    "        sample_idx = np.random.choice(len(self.df_train), sample_size, replace=False)\n",
    "        X_sample = self.df_train.iloc[sample_idx]\n",
    "        \n",
    "        try:\n",
    "            # Calculate SHAP values\n",
    "            explainer = shap.TreeExplainer(self.model)\n",
    "            shap_values = explainer.shap_values(X_sample)\n",
    "            \n",
    "            # Get feature importance\n",
    "            feature_importance = np.abs(shap_values).mean(axis=0)\n",
    "            feature_scores = pd.Series(feature_importance, index=X_sample.columns)\n",
    "            \n",
    "            # Select top features\n",
    "            top_features = feature_scores.nlargest(top_k).index.tolist()\n",
    "            \n",
    "            print(f\"Selected {len(top_features)} features out of {len(self.feature_columns)}\")\n",
    "            \n",
    "            # Update datasets\n",
    "            self.df_train = self.df_train[top_features]\n",
    "            self.df_test = self.df_test[top_features]\n",
    "            self.feature_columns = top_features\n",
    "            \n",
    "            return top_features\n",
    "        except Exception as e:\n",
    "            print(f\"SHAP failed, using all features: {e}\")\n",
    "            return self.feature_columns[:top_k]\n",
    "    \n",
    "    def train_final_model(self):\n",
    "        \"\"\"Train final model on selected features\"\"\"\n",
    "        print(\"Training final model...\")\n",
    "        \n",
    "        # Parameters for final model\n",
    "        params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'learning_rate': 0.03,\n",
    "            'max_depth': 10,\n",
    "            'min_child_weight': 1,\n",
    "            'gamma': 0.1,\n",
    "            'subsample': 0.9,\n",
    "            'colsample_bytree': 0.9,\n",
    "            'reg_alpha': 0.1,\n",
    "            'reg_lambda': 1.0,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = []\n",
    "        kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kfold.split(self.df_train, self.y_train)):\n",
    "            X_train_fold = self.df_train.iloc[train_idx]\n",
    "            X_val_fold = self.df_train.iloc[val_idx]\n",
    "            y_train_fold = self.y_train.iloc[train_idx]\n",
    "            y_val_fold = self.y_train.iloc[val_idx]\n",
    "            \n",
    "            dtrain = xgb.DMatrix(X_train_fold, label=y_train_fold)\n",
    "            dval = xgb.DMatrix(X_val_fold, label=y_val_fold)\n",
    "            \n",
    "            model_fold = xgb.train(\n",
    "                params,\n",
    "                dtrain,\n",
    "                num_boost_round=3000,\n",
    "                evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "                early_stopping_rounds=150,\n",
    "                verbose_eval=False\n",
    "            )\n",
    "            \n",
    "            y_pred = model_fold.predict(dval)\n",
    "            auc_score = roc_auc_score(y_val_fold, y_pred)\n",
    "            cv_scores.append(auc_score)\n",
    "            print(f\"Fold {fold+1} AUC: {auc_score:.4f}\")\n",
    "        \n",
    "        print(f\"CV AUC: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores)*2:.4f})\")\n",
    "        \n",
    "        # Train final model on full data\n",
    "        dtrain_full = xgb.DMatrix(self.df_train, label=self.y_train)\n",
    "        \n",
    "        self.model = xgb.train(\n",
    "            params,\n",
    "            dtrain_full,\n",
    "            num_boost_round=int(np.mean([model.best_iteration for model in [model_fold]]) * 1.1),\n",
    "            verbose_eval=100\n",
    "        )\n",
    "        \n",
    "        return np.mean(cv_scores)\n",
    "    \n",
    "    def predict(self):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        print(\"Making predictions...\")\n",
    "        \n",
    "        dtest = xgb.DMatrix(self.df_test)\n",
    "        predictions = self.model.predict(dtest)\n",
    "        \n",
    "        # Create submission\n",
    "        submission = pd.DataFrame({\n",
    "            'id1': self.ids_test['id1'],\n",
    "            'id2': self.ids_test['id2'],\n",
    "            'id3': self.ids_test['id3'],\n",
    "            'id5': self.ids_test['id5'],\n",
    "            'pred': predictions\n",
    "        })\n",
    "        \n",
    "        return submission\n",
    "    \n",
    "    def create_submission(self, predictions_df):\n",
    "        \"\"\"Create final submission file\"\"\"\n",
    "        print(\"Creating submission...\")\n",
    "        \n",
    "        # Map predictions to submission template\n",
    "        pred_map = dict(zip(predictions_df['id1'], predictions_df['pred']))\n",
    "        \n",
    "        # Load submission template\n",
    "        submission = self.df_submission.copy()\n",
    "        submission['y'] = submission['id1'].map(pred_map).fillna(0.5).clip(0, 1)\n",
    "        \n",
    "        # Save submission\n",
    "        submission.to_csv('final_submission.csv', index=False)\n",
    "        print(f\"Submission saved with {len(submission)} rows\")\n",
    "        \n",
    "        return submission\n",
    "    \n",
    "    def run_pipeline(self):\n",
    "        \"\"\"Run complete pipeline\"\"\"\n",
    "        print(\"Starting complete pipeline...\")\n",
    "        \n",
    "        # Load data\n",
    "        self.load_data()\n",
    "        \n",
    "        # Prepare data\n",
    "        self.prepare_data()\n",
    "        \n",
    "        # Train initial model\n",
    "        initial_auc = self.train_model()\n",
    "        \n",
    "        # Feature selection\n",
    "        selected_features = self.feature_selection(top_k=400)\n",
    "        \n",
    "        # Train final model\n",
    "        final_auc = self.train_final_model()\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = self.predict()\n",
    "        \n",
    "        # Create submission\n",
    "        submission = self.create_submission(predictions)\n",
    "        \n",
    "        print(f\"Pipeline complete!\")\n",
    "        print(f\"Initial AUC: {initial_auc:.4f}\")\n",
    "        print(f\"Final AUC: {final_auc:.4f}\")\n",
    "        print(f\"Submission shape: {submission.shape}\")\n",
    "        \n",
    "        return submission\n",
    "\n",
    "# Run the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    model = ClickPredictionModel()\n",
    "    submission = model.run_pipeline()\n",
    "    \n",
    "    # Display prediction distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(submission['y'], bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.title('Distribution of Predicted Click Probabilities')\n",
    "    plt.xlabel('Predicted Probability')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Prediction statistics:\")\n",
    "    print(f\"Mean: {submission['y'].mean():.4f}\")\n",
    "    print(f\"Std: {submission['y'].std():.4f}\")\n",
    "    print(f\"Min: {submission['y'].min():.4f}\")\n",
    "    print(f\"Max: {submission['y'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e02a3-d6ab-4672-b62c-9af1e9c08d61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
