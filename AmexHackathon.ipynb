{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecaad478-5acb-4161-87f4-499f05a674d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import joblib\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba9fee0a-9848-414a-ab32-b4b03ca1130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClickPredictionModel:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.feature_columns = None\n",
    "        self.label_encoders = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load all datasets\"\"\"\n",
    "        # Load main datasets\n",
    "        self.df_events = pd.read_parquet('/Users/jatin/Desktop/AMEX Hackathon/add_event.parquet')\n",
    "        self.df_trans = pd.read_parquet('/Users/jatin/Desktop/AMEX Hackathon/add_trans.parquet')\n",
    "        self.df_offers = pd.read_parquet('/Users/jatin/Desktop/AMEX Hackathon/offer_metadata.parquet')\n",
    "        self.df_train = pd.read_parquet('/Users/jatin/Desktop/AMEX Hackathon/train_data.parquet')\n",
    "        self.df_test = pd.read_parquet('/Users/jatin/Desktop/AMEX Hackathon/test_data.parquet')\n",
    "        self.df_submission = pd.read_csv('/Users/jatin/Desktop/AMEX Hackathon/submission_template.csv')\n",
    "        \n",
    "        # Ensure ID columns are consistent - FIXED: Convert all ID columns to string\n",
    "        for df in [self.df_events, self.df_trans, self.df_offers, self.df_train, self.df_test]:\n",
    "            id_columns = [col for col in df.columns if col.startswith('id')]\n",
    "            for col in id_columns:\n",
    "                df[col] = df[col].astype(str)\n",
    "        \n",
    "        print(\"Data loaded successfully!\")\n",
    "        print(f\"Events shape: {self.df_events.shape}\")\n",
    "        print(f\"Transactions shape: {self.df_trans.shape}\")\n",
    "        print(f\"Offers shape: {self.df_offers.shape}\")\n",
    "        print(f\"Train shape: {self.df_train.shape}\")\n",
    "        print(f\"Test shape: {self.df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265acbce-383a-48e9-92d5-877daf877d56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee6d9531-d5a9-45c9-931b-05172ed54363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_event_features(self, df):\n",
    "        \"\"\"Create features from events data\"\"\"\n",
    "        if hasattr(self, 'df_events') and not self.df_events.empty:\n",
    "            # Convert timestamp columns to datetime safely\n",
    "            try:\n",
    "                self.df_events['id4'] = pd.to_datetime(self.df_events['id4'], errors='coerce')\n",
    "                self.df_events['id7'] = pd.to_datetime(self.df_events['id7'], errors='coerce')\n",
    "                self.df_events['timestamp_diff'] = (self.df_events['id7'] - self.df_events['id4']).dt.total_seconds()\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing timestamps: {e}\")\n",
    "                self.df_events['timestamp_diff'] = 0\n",
    "            \n",
    "            # Basic event aggregations\n",
    "            agg_dict = {\n",
    "                'id3': ['count', 'nunique'],\n",
    "                'timestamp_diff': ['min', 'max', 'mean', 'std'],\n",
    "                'id6': 'nunique'\n",
    "            }\n",
    "            \n",
    "            valid_columns = {k: v for k, v in agg_dict.items() if k in self.df_events.columns}\n",
    "            if valid_columns:\n",
    "                events_agg = self.df_events.groupby('id2').agg(valid_columns).reset_index()\n",
    "                \n",
    "                # Flatten column names\n",
    "                events_agg.columns = ['id2'] + [f'event_{col[0]}_{col[1]}' for col in events_agg.columns[1:]]\n",
    "                \n",
    "                # Calculate event-based ratios safely\n",
    "                if 'event_id3_count' in events_agg.columns:\n",
    "                    events_agg['event_click_rate'] = events_agg['event_id3_count'] / (events_agg['event_id3_count'] + 1e-6)\n",
    "                \n",
    "                # Merge with main dataframe\n",
    "                if 'id2' in df.columns:\n",
    "                    df = df.merge(events_agg, on='id2', how='left')\n",
    "            \n",
    "            # Customer-offer specific features\n",
    "            if all(col in self.df_events.columns for col in ['id2', 'id3', 'timestamp_diff', 'id6']):\n",
    "                customer_offer_agg = {\n",
    "                    'timestamp_diff': ['count', 'mean'],\n",
    "                    'id6': 'nunique'\n",
    "                }\n",
    "                \n",
    "                customer_offer_events = self.df_events.groupby(['id2', 'id3']).agg(customer_offer_agg).reset_index()\n",
    "                customer_offer_events.columns = ['id2', 'id3'] + [f'co_event_{col[0]}_{col[1]}' for col in customer_offer_events.columns[2:]]\n",
    "                \n",
    "                if 'co_event_timestamp_diff_count' in customer_offer_events.columns:\n",
    "                    customer_offer_events['co_event_specific_ctr'] = (\n",
    "                        customer_offer_events['co_event_timestamp_diff_count'] / \n",
    "                        (customer_offer_events['co_event_timestamp_diff_count'] + 1e-6)\n",
    "                    )\n",
    "                \n",
    "                if all(col in df.columns for col in ['id2', 'id3']):\n",
    "                    df = df.merge(customer_offer_events, on=['id2', 'id3'], how='left')\n",
    "                    \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa33f26-4536-4a01-a1d2-7ea5b8ce9bed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01bb9c17-5729-4310-9230-edd1a9161102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transaction_features(self, df):\n",
    "        \"\"\"Create features from transaction data\"\"\"\n",
    "        if hasattr(self, 'df_trans') and not self.df_trans.empty:\n",
    "            # Convert numeric columns to numeric type\n",
    "            numeric_cols = ['f367', 'f368', 'f369', 'f370', 'f371', 'f372', 'f374']\n",
    "            for col in numeric_cols:\n",
    "                if col in self.df_trans.columns:\n",
    "                    self.df_trans[col] = pd.to_numeric(self.df_trans[col], errors='coerce')\n",
    "            \n",
    "            # Handle non-numeric columns safely\n",
    "            if 'id8' in self.df_trans.columns:\n",
    "                self.df_trans['id8'] = self.df_trans['id8'].astype(str)\n",
    "            \n",
    "            # Aggregate transaction features by customer\n",
    "            agg_dict = {\n",
    "                'f367': ['sum', 'mean', 'count', 'std', 'min', 'max'],\n",
    "                'f368': ['nunique', 'count'],\n",
    "                'f369': ['sum', 'mean'],\n",
    "                'f370': ['min', 'max', 'nunique'],\n",
    "                'f371': ['mean', 'std'],\n",
    "                'f372': ['nunique'],\n",
    "                'f374': ['nunique'],\n",
    "                'id8': ['nunique']\n",
    "            }\n",
    "            \n",
    "            # Only include columns that exist in the dataframe\n",
    "            valid_columns = {k: v for k, v in agg_dict.items() if k in self.df_trans.columns}\n",
    "            if valid_columns:\n",
    "                trans_agg = self.df_trans.groupby('id2').agg(valid_columns).reset_index()\n",
    "                \n",
    "                # Flatten column names\n",
    "                trans_agg.columns = ['id2'] + [f'trans_{col[0]}_{col[1]}' for col in trans_agg.columns[1:]]\n",
    "                \n",
    "                # Create advanced transaction features safely\n",
    "                if 'trans_f367_sum' in trans_agg.columns and 'trans_f367_count' in trans_agg.columns:\n",
    "                    trans_agg['trans_avg_amount'] = trans_agg['trans_f367_sum'] / (trans_agg['trans_f367_count'] + 1e-6)\n",
    "                \n",
    "                if 'trans_f367_std' in trans_agg.columns and 'trans_f367_mean' in trans_agg.columns:\n",
    "                    trans_agg['trans_amount_cv'] = trans_agg['trans_f367_std'] / (trans_agg['trans_f367_mean'] + 1e-6)\n",
    "                \n",
    "                if 'trans_f367_max' in trans_agg.columns and 'trans_f367_mean' in trans_agg.columns:\n",
    "                    trans_agg['trans_high_value_ratio'] = (trans_agg['trans_f367_max'] > trans_agg['trans_f367_mean'] * 2).astype(int)\n",
    "                \n",
    "                if 'trans_f368_nunique' in trans_agg.columns and 'trans_f368_count' in trans_agg.columns:\n",
    "                    trans_agg['trans_product_diversity'] = trans_agg['trans_f368_nunique'] / (trans_agg['trans_f368_count'] + 1e-6)\n",
    "                \n",
    "                if 'trans_amount_cv' in trans_agg.columns:\n",
    "                    trans_agg['trans_spending_consistency'] = 1 / (trans_agg['trans_amount_cv'] + 1e-6)\n",
    "                \n",
    "                # Merge with main dataframe\n",
    "                if 'id2' in df.columns:\n",
    "                    df = df.merge(trans_agg, on='id2', how='left')\n",
    "                    \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cff4696-fb57-4d60-b3a9-18a008a6f7bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8b86b34-7632-4881-b3f5-9d3aa03bcb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_offer_features(self, df):\n",
    "        \"\"\"Create features from offer metadata\"\"\"\n",
    "        if hasattr(self, 'df_offers') and not self.df_offers.empty:\n",
    "            # Ensure numeric columns are numeric\n",
    "            numeric_cols = ['f375', 'f376', 'f377', 'f378', 'f374']\n",
    "            for col in numeric_cols:\n",
    "                if col in self.df_offers.columns:\n",
    "                    self.df_offers[col] = pd.to_numeric(self.df_offers[col], errors='coerce')\n",
    "            \n",
    "            # Convert datetime columns to datetime objects first\n",
    "            datetime_cols = ['id12', 'id13']\n",
    "            for col in datetime_cols:\n",
    "                if col in self.df_offers.columns:\n",
    "                    self.df_offers[col] = pd.to_datetime(self.df_offers[col], errors='coerce')\n",
    "            \n",
    "            # Define aggregations - separate numeric and datetime aggregations\n",
    "            numeric_agg_dict = {\n",
    "                'f375': ['mean', 'std'],\n",
    "                'f376': ['mean', 'std'],\n",
    "                'f377': ['mean', 'std'],\n",
    "                'id10': ['nunique'],\n",
    "                'id11': ['nunique'],\n",
    "                'f378': ['mean', 'std'],\n",
    "                'f374': ['nunique']\n",
    "            }\n",
    "            \n",
    "            datetime_agg_dict = {\n",
    "                'id12': ['min', 'max'],\n",
    "                'id13': ['min', 'max']\n",
    "            }\n",
    "            \n",
    "            # Only include columns that exist\n",
    "            valid_numeric_cols = {k: v for k, v in numeric_agg_dict.items() \n",
    "                                if k in self.df_offers.columns}\n",
    "            valid_datetime_cols = {k: v for k, v in datetime_agg_dict.items() \n",
    "                                 if k in self.df_offers.columns}\n",
    "            \n",
    "            # Perform aggregations if we have valid columns\n",
    "            if valid_numeric_cols or valid_datetime_cols:\n",
    "                # Start with numeric aggregations\n",
    "                if valid_numeric_cols:\n",
    "                    numeric_agg = self.df_offers.groupby('id3').agg(valid_numeric_cols).reset_index()\n",
    "                    # Flatten column names\n",
    "                    numeric_agg.columns = ['id3'] + [f'offer_{col[0]}_{col[1]}' for col in numeric_agg.columns[1:]]\n",
    "                else:\n",
    "                    numeric_agg = self.df_offers[['id3']].drop_duplicates()\n",
    "                \n",
    "                # Handle datetime aggregations\n",
    "                if valid_datetime_cols:\n",
    "                    datetime_agg = self.df_offers.groupby('id3').agg(valid_datetime_cols).reset_index()\n",
    "                    # Flatten column names\n",
    "                    datetime_agg.columns = ['id3'] + [f'offer_{col[0]}_{col[1]}' for col in datetime_agg.columns[1:]]\n",
    "                    \n",
    "                    # Calculate datetime differences if we have both min and max\n",
    "                    if 'offer_id13_max' in datetime_agg.columns and 'offer_id12_min' in datetime_agg.columns:\n",
    "                        datetime_agg['offer_duration'] = (\n",
    "                            datetime_agg['offer_id13_max'] - datetime_agg['offer_id12_min']\n",
    "                        ).dt.total_seconds()\n",
    "                    \n",
    "                    # Merge the aggregations\n",
    "                    offer_agg = numeric_agg.merge(datetime_agg, on='id3', how='left')\n",
    "                else:\n",
    "                    offer_agg = numeric_agg\n",
    "                \n",
    "                # Create offer-specific features safely\n",
    "                if 'offer_f376_mean' in offer_agg.columns and 'offer_f375_mean' in offer_agg.columns:\n",
    "                    offer_agg['offer_attractiveness'] = (\n",
    "                        offer_agg['offer_f376_mean'] * offer_agg['offer_f375_mean']\n",
    "                    )\n",
    "                \n",
    "                if 'offer_f378_std' in offer_agg.columns and 'offer_f378_mean' in offer_agg.columns:\n",
    "                    offer_agg['offer_complexity'] = (\n",
    "                        offer_agg['offer_f378_std'] / (offer_agg['offer_f378_mean'] + 1e-6))\n",
    "                \n",
    "                # Merge with main dataframe\n",
    "                if 'id3' in df.columns:\n",
    "                    df = df.merge(offer_agg, on='id3', how='left')\n",
    "                    \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab94a0ea-7165-483f-a785-686d7c9cfa3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c669379-61e8-456a-b777-c211e35722a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_features(self, df):\n",
    "        \"\"\"Create advanced engineered features\"\"\"\n",
    "        # Make a copy to avoid SettingWithCopyWarning\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Time-based features\n",
    "        if 'id5' in df.columns:\n",
    "            df['id5_numeric'] = pd.to_numeric(df['id5'], errors='coerce')\n",
    "            df['day_of_week'] = df['id5_numeric'] % 7\n",
    "            df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "        \n",
    "        if 'id4' in df.columns:\n",
    "            df['id4_numeric'] = pd.to_numeric(df['id4'], errors='coerce')\n",
    "            df['hour_of_day'] = (df['id4_numeric'] % 86400) // 3600\n",
    "            df['is_business_hours'] = ((df['hour_of_day'] >= 9) & (df['hour_of_day'] <= 17)).astype(int)\n",
    "        \n",
    "        # Customer engagement features\n",
    "        engagement_features = [f for f in ['f28', 'f29', 'f30', 'f31', 'f147', 'f148', 'f149', 'f150'] \n",
    "                             if f in df.columns]\n",
    "        if engagement_features:\n",
    "            df['total_engagement'] = df[engagement_features].sum(axis=1)\n",
    "            df['avg_engagement'] = df[engagement_features].mean(axis=1)\n",
    "            df['engagement_consistency'] = df[engagement_features].std(axis=1)\n",
    "        \n",
    "        # CTR-based features\n",
    "        ctr_features = [col for col in df.columns \n",
    "                       if 'ctr' in col.lower() or \n",
    "                       (col.startswith('f') and len(col) > 1 and col[1:].isdigit() and \n",
    "                        int(col[1:]) in range(4, 10))]\n",
    "        if ctr_features:\n",
    "            df['avg_ctr'] = df[ctr_features].mean(axis=1)\n",
    "            df['max_ctr'] = df[ctr_features].max(axis=1)\n",
    "            df['ctr_consistency'] = df[ctr_features].std(axis=1)\n",
    "        \n",
    "        # Spending pattern features\n",
    "        spending_features = [f for f in ['f39', 'f40', 'f41'] if f in df.columns]\n",
    "        if spending_features:\n",
    "            df['total_spending'] = df[spending_features].sum(axis=1)\n",
    "            df['spending_diversity'] = (df[spending_features] > 0).sum(axis=1)\n",
    "            if len(spending_features) > 0:\n",
    "                df['dominant_spending_category'] = df[spending_features].idxmax(axis=1)\n",
    "        \n",
    "        # Interaction features with existence checks\n",
    "        interaction_pairs = [\n",
    "            ('f363', 'f331'), ('f366', 'f329'), ('f150', 'f329'),\n",
    "            ('f138', 'f22'), ('f132', 'f68'), ('f363', 'f366')\n",
    "        ]\n",
    "        \n",
    "        for f1, f2 in interaction_pairs:\n",
    "            if all(f in df.columns for f in [f1, f2]):\n",
    "                df[f'{f1}_x_{f2}'] = df[f1] * df[f2]\n",
    "        \n",
    "        # Ratio features\n",
    "        ratio_pairs = [\n",
    "            ('f363', 'f329'), ('f366', 'f329'), ('f214', 'f22')\n",
    "        ]\n",
    "        \n",
    "        for f1, f2 in ratio_pairs:\n",
    "            if all(f in df.columns for f in [f1, f2]):\n",
    "                df[f'{f1}_div_{f2}'] = df[f1] / (df[f2] + 1e-6)\n",
    "        \n",
    "        # Higher order features\n",
    "        higher_order_features = ['f132', 'f363', 'f366', 'f351']\n",
    "        for feature in higher_order_features:\n",
    "            if feature in df.columns:\n",
    "                df[f'{feature}_sq'] = df[feature] ** 2\n",
    "                if feature != 'f351':\n",
    "                    df[f'{feature}_log'] = np.log1p(df[feature].clip(lower=0))\n",
    "                if feature == 'f366':\n",
    "                    df[f'{feature}_inv'] = 1.0 / (df[feature] + 1e-6)\n",
    "        \n",
    "        # Binning important features\n",
    "        binning_features = [('f363', [0.1, 0.25]), ('f366', [0.1, 0.3])]\n",
    "        for feature, thresholds in binning_features:\n",
    "            if feature in df.columns:\n",
    "                df[f'{feature}_bin'] = pd.cut(df[feature], \n",
    "                                            [-np.inf] + thresholds + [np.inf], \n",
    "                                            labels=range(len(thresholds) + 1)).astype(int)\n",
    "        \n",
    "        # Customer value segmentation\n",
    "        value_features = [f for f in ['f43', 'f44', 'f47', 'f49'] if f in df.columns]\n",
    "        if value_features:\n",
    "            df['customer_value_score'] = df[value_features].sum(axis=1)\n",
    "            if len(df) > 0:\n",
    "                df['is_high_value_customer'] = (df['customer_value_score'] > \n",
    "                                               df['customer_value_score'].quantile(0.8)).astype(int)\n",
    "        \n",
    "        # Offer-specific customer features\n",
    "        if all(f in df.columns for f in ['f363', 'f366', 'f150']):\n",
    "            df['customer_offer_match_score'] = df['f363'] * df['f366'] * df['f150']\n",
    "        \n",
    "        if 'f223' in df.columns:\n",
    "            df['recency_score'] = np.exp(-df['f223'] / 30)\n",
    "        \n",
    "        # PCA features\n",
    "        low_imp_features = [col for col in df.columns \n",
    "                          if col.startswith('f') and len(col) > 1 and col[1:].isdigit() and \n",
    "                          int(col[1:]) > 300]\n",
    "        if len(low_imp_features) > 1:\n",
    "            try:\n",
    "                pca = PCA(n_components=min(5, len(low_imp_features)))\n",
    "                df_pca = pca.fit_transform(df[low_imp_features].fillna(0))\n",
    "                for i in range(df_pca.shape[1]):\n",
    "                    df[f'pca_component_{i}'] = df_pca[:, i]\n",
    "            except Exception as e:\n",
    "                print(f\"PCA failed: {e}\")\n",
    "        \n",
    "        # Clustering features\n",
    "        cluster_features = [f for f in ['f363', 'f366', 'f150', 'f138'] if f in df.columns]\n",
    "        if len(cluster_features) >= 2:\n",
    "            try:\n",
    "                n_clusters = min(5, len(df)//10)\n",
    "                if n_clusters > 1:\n",
    "                    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "                    df['customer_cluster'] = kmeans.fit_predict(df[cluster_features].fillna(0))\n",
    "            except Exception as e:\n",
    "                print(f\"Clustering failed: {e}\")\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b57904-74a1-4f3d-9077-3dd9b82ac13f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "520503ca-473f-4c31-8b34-9a91293e29f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_preprocessing(self, df):\n",
    "        \"\"\"Basic preprocessing steps\"\"\"\n",
    "        # Convert ID columns to consistent types\n",
    "        id_cols = ['id1', 'id2', 'id3', 'id4', 'id5']\n",
    "        for col in id_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(str)\n",
    "        \n",
    "        # Convert other columns to numeric\n",
    "        for col in df.columns:\n",
    "            if col not in id_cols and col != 'y':\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        # Handle missing values\n",
    "        df.fillna(0, inplace=True)\n",
    "        \n",
    "        # Handle infinite values\n",
    "        df.replace([np.inf, -np.inf], 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a98c94-fd9c-4949-85f6-5be855db5aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30c64fab-6427-49eb-8db3-71a125ec8863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(self, df):\n",
    "        \"\"\"Complete feature engineering pipeline\"\"\"\n",
    "        print(\"Starting feature engineering...\")\n",
    "        \n",
    "        # Basic preprocessing\n",
    "        df = self.basic_preprocessing(df)\n",
    "        \n",
    "        # Create features from additional datasets\n",
    "        df = self.create_event_features(df)\n",
    "        df = self.create_transaction_features(df)\n",
    "        df = self.create_offer_features(df)\n",
    "        \n",
    "        # Create advanced features\n",
    "        df = self.create_advanced_features(df)\n",
    "        \n",
    "        # Final preprocessing\n",
    "        df = self.basic_preprocessing(df)\n",
    "        \n",
    "        print(f\"Feature engineering complete. Shape: {df.shape}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b6d787-1e9c-45cf-b5b5-b735703b1aba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "377a67d8-0f79-4204-9b28-caf2d13c2f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(self):\n",
    "        \"\"\"Train the XGBoost model with cross-validation\"\"\"\n",
    "        print(\"Training model...\")\n",
    "        \n",
    "        # Split for validation\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            self.df_train, self.y_train, test_size=0.2, random_state=42, stratify=self.y_train\n",
    "        )\n",
    "        \n",
    "        # XGBoost parameters optimized for ranking\n",
    "        params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'learning_rate': 0.05,\n",
    "            'max_depth': 8,\n",
    "            'min_child_weight': 1,\n",
    "            'gamma': 0.1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'reg_alpha': 0.1,\n",
    "            'reg_lambda': 1.0,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        # Prepare DMatrix\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dval = xgb.DMatrix(X_val, label=y_val)\n",
    "        \n",
    "        # Train model\n",
    "        self.model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=2000,\n",
    "            evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval=100\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        y_pred = self.model.predict(dval)\n",
    "        auc_score = roc_auc_score(y_val, y_pred)\n",
    "        print(f\"Validation AUC: {auc_score:.4f}\")\n",
    "        \n",
    "        return auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05ddbcd-c9d9-4748-8793-be568e96e08b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69a073c4-21f6-49d1-be45-d5f594b07ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(self, top_k=500):\n",
    "        \"\"\"Feature selection using SHAP\"\"\"\n",
    "        print(\"Performing feature selection...\")\n",
    "        \n",
    "        # Sample for SHAP calculation\n",
    "        sample_size = min(1000, len(self.df_train))\n",
    "        if sample_size == 0:\n",
    "            return self.feature_columns[:top_k]\n",
    "            \n",
    "        sample_idx = np.random.choice(len(self.df_train), sample_size, replace=False)\n",
    "        X_sample = self.df_train.iloc[sample_idx]\n",
    "        \n",
    "        try:\n",
    "            # Calculate SHAP values\n",
    "            explainer = shap.TreeExplainer(self.model)\n",
    "            shap_values = explainer.shap_values(X_sample)\n",
    "            \n",
    "            # Get feature importance\n",
    "            feature_importance = np.abs(shap_values).mean(axis=0)\n",
    "            feature_scores = pd.Series(feature_importance, index=X_sample.columns)\n",
    "            \n",
    "            # Select top features\n",
    "            top_features = feature_scores.nlargest(top_k).index.tolist()\n",
    "            \n",
    "            print(f\"Selected {len(top_features)} features out of {len(self.feature_columns)}\")\n",
    "            \n",
    "            # Update datasets\n",
    "            self.df_train = self.df_train[top_features]\n",
    "            self.df_test = self.df_test[top_features]\n",
    "            self.feature_columns = top_features\n",
    "            \n",
    "            return top_features\n",
    "        except Exception as e:\n",
    "            print(f\"SHAP failed, using all features: {e}\")\n",
    "            return self.feature_columns[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fd1635-13bc-46d8-afb2-8c7a236c4f96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e6b5eb-35e4-4b43-8136-7fc9cde49ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final_model(self):\n",
    "    \"\"\"Train final model on selected features\"\"\"\n",
    "    print(\"Training final model...\")\n",
    "    \n",
    "    # Parameters for final model\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'learning_rate': 0.03,\n",
    "        'max_depth': 10,\n",
    "        'min_child_weight': 1,\n",
    "        'gamma': 0.1,\n",
    "        'subsample': 0.9,\n",
    "        'colsample_bytree': 0.9,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 1.0,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = []\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(self.df_train, self.y_train)):\n",
    "        X_train_fold = self.df_train.iloc[train_idx]\n",
    "        X_val_fold = self.df_train.iloc[val_idx]\n",
    "        y_train_fold = self.y_train.iloc[train_idx]\n",
    "        y_val_fold = self.y_train.iloc[val_idx]\n",
    "        \n",
    "        dtrain = xgb.DMatrix(X_train_fold, label=y_train_fold)\n",
    "        dval = xgb.DMatrix(X_val_fold, label=y_val_fold)\n",
    "        \n",
    "        model_fold = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=3000,\n",
    "            evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "            early_stopping_rounds=150,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        \n",
    "        y_pred = model_fold.predict(dval)\n",
    "        auc_score = roc_auc_score(y_val_fold, y_pred)\n",
    "        cv_scores.append(auc_score)\n",
    "        print(f\"Fold {fold+1} AUC: {auc_score:.4f}\")\n",
    "    \n",
    "    print(f\"CV AUC: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores)*2:.4f})\")\n",
    "    \n",
    "    # Train final model on full data\n",
    "    dtrain_full = xgb.DMatrix(self.df_train, label=self.y_train)\n",
    "    \n",
    "    self.model = xgb.train(\n",
    "        params,\n",
    "        dtrain_full,\n",
    "        num_boost_round=int(np.mean([model_fold.best_iteration]) * 1.1),\n",
    "        verbose_eval=100\n",
    "    )\n",
    "    \n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "# 13. Prediction\n",
    "def predict(self):\n",
    "    \"\"\"Make predictions\"\"\"\n",
    "    print(\"Making predictions...\")\n",
    "    \n",
    "    dtest = xgb.DMatrix(self.df_test)\n",
    "    predictions = self.model.predict(dtest)\n",
    "    \n",
    "    # Create submission\n",
    "    submission = pd.DataFrame({\n",
    "        'id1': self.ids_test['id1'],\n",
    "        'id2': self.ids_test['id2'],\n",
    "        'id3': self.ids_test['id3'],\n",
    "        'id5': self.ids_test['id5'],\n",
    "        'pred': predictions\n",
    "    })\n",
    "    \n",
    "    return submission\n",
    "\n",
    "# 14. Submission Creation\n",
    "def create_submission(self, predictions_df):\n",
    "    \"\"\"Create final submission file\"\"\"\n",
    "    print(\"Creating submission...\")\n",
    "    \n",
    "    # Map predictions to submission template\n",
    "    pred_map = dict(zip(predictions_df['id1'], predictions_df['pred']))\n",
    "    \n",
    "    # Load submission template\n",
    "    submission = self.df_submission.copy()\n",
    "    submission['y'] = submission['id1'].map(pred_map).fillna(0.5).clip(0, 1)\n",
    "    \n",
    "    # Save submission\n",
    "    submission.to_csv('final_submission.csv', index=False)\n",
    "    print(f\"Submission saved with {len(submission)} rows\")\n",
    "    \n",
    "    return submission\n",
    "\n",
    "# 15. Complete Pipeline\n",
    "def run_pipeline(self):\n",
    "    \"\"\"Run complete pipeline\"\"\"\n",
    "    print(\"Starting complete pipeline...\")\n",
    "    \n",
    "    # Load data\n",
    "    self.load_data()\n",
    "    \n",
    "    # Prepare data\n",
    "    self.prepare_data()\n",
    "    \n",
    "    # Train initial model\n",
    "    initial_auc = self.train_model()\n",
    "    \n",
    "    # Feature selection\n",
    "    selected_features = self.feature_selection(top_k=400)\n",
    "    \n",
    "    # Train final model\n",
    "    final_auc = self.train_final_model()\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = self.predict()\n",
    "    \n",
    "    # Create submission\n",
    "    submission = self.create_submission(predictions)\n",
    "    \n",
    "    print(f\"Pipeline complete!\")\n",
    "    print(f\"Initial AUC: {initial_auc:.4f}\")\n",
    "    print(f\"Final AUC: {final_auc:.4f}\")\n",
    "    print(f\"Submission shape: {submission.shape}\")\n",
    "    \n",
    "    return submission\n",
    "\n",
    "# 16. Main Execution and Visualization\n",
    "# Run the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    model = ClickPredictionModel()\n",
    "    submission = model.run_pipeline()\n",
    "    \n",
    "    # Display prediction distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(submission['y'], bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.title('Distribution of Predicted Click Probabilities')\n",
    "    plt.xlabel('Predicted Probability')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Prediction statistics:\")\n",
    "    print(f\"Mean: {submission['y'].mean():.4f}\")\n",
    "    print(f\"Std: {submission['y'].std():.4f}\")\n",
    "    print(f\"Min: {submission['y'].min():.4f}\")\n",
    "    print(f\"Max: {submission['y'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4ac7e5-41a0-46cf-86d0-acd8e5f3ea60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b09c7e-93bb-40b2-ae00-f446dbe83a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
